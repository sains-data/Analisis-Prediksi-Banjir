services:
  # ==================== STORAGE LAYER (LATEST HADOOP 3.4.1) ====================
  namenode:
    image: apache/hadoop:3.4.1
    container_name: namenode
    restart: always
    user: "root"
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
      - ./data:/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - CLUSTER_NAME=lampung-flood-cluster
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop

  datanode:
    image: apache/hadoop:3.4.1
    container_name: datanode
    restart: always
    user: "root"
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/opt/hadoop/dfs/data
      - ./data:/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
    networks:
      - hadoop
    depends_on:
      - namenode

  resourcemanager:
    image: apache/hadoop:3.4.1
    container_name: resourcemanager
    restart: always
    user: "root"
    command: ["yarn", "resourcemanager"]
    ports:
      - "8088:8088"
    volumes:
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode

  nodemanager:
    image: apache/hadoop:3.4.1
    container_name: nodemanager
    restart: always
    user: "root"
    command: ["yarn", "nodemanager"]
    ports:
      - "8042:8042"
    volumes:
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop
    depends_on:
      - resourcemanager

  historyserver:
    image: apache/hadoop:3.4.1
    container_name: historyserver
    restart: always
    user: "root"
    command: ["mapred", "historyserver"]
    ports:
      - "8188:8188"
    volumes:
      - hadoop_historyserver:/opt/hadoop/logs/history
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - MAPRED_HISTORYSERVER_USER=root
    networks:
      - hadoop
    depends_on:
      - resourcemanager

  # ==================== STREAMING LAYER (LATEST KAFKA & ZOOKEEPER) ====================
  zookeeper:
    image: zookeeper:3.9
    container_name: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888;2181
      ZOO_TICK_TIME: 2000
      ZOO_INIT_LIMIT: 10
      ZOO_SYNC_LIMIT: 5
    volumes:
      - zookeeper_data:/apache-zookeeper-3.9-bin/data
      - zookeeper_logs:/apache-zookeeper-3.9-bin/logs
    networks:
      - hadoop

  kafka:
    image: apache/kafka:3.9.1
    container_name: kafka
    restart: always
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      CLUSTER_ID: "5L6g3nShT-eMCtK--X86sw"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - hadoop

  # ==================== PROCESSING LAYER (LATEST SPARK 3.5.4) ====================
  spark-master:
    image: apache/spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-master
    restart: always
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - namenode

  spark-worker-1:
    image: apache/spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-worker-1
    restart: always
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8881
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - spark-master

  # ==================== SERVING LAYER (LATEST HIVE 4.0.1) ====================
  hive-server:
    image: apache/hive:4.0.1
    container_name: hive-server
    hostname: hive-server
    restart: always
    ports:
      - "9083:9083"
      - "10000:10000"
      - "10002:10002"
    environment:
      SERVICE_NAME: 'hiveserver2'
      DB_DRIVER: derby
      SKIP_SCHEMA_INIT: 'false'
    volumes:
      - ./config/hive/simple-hive-site.xml:/opt/hive/conf/hive-site.xml
    networks:
      - hadoop
    depends_on:
      - namenode

  # ==================== NOSQL LAYER (LATEST HBASE 2.6.1) ====================
  hbase-master:
    image: harisekhon/hbase:latest
    container_name: hbase-master
    restart: always
    ports:
      - "16010:16010"
      - "16000:16000"
    environment:
      HBASE_CONF_hbase_rootdir: hdfs://namenode:8020/hbase
      HBASE_CONF_hbase_zookeeper_quorum: zookeeper
      HBASE_CONF_hbase_zookeeper_property_clientPort: 2181
      HBASE_CONF_hbase_cluster_distributed: "true"
      HBASE_CONF_hbase_master_hostname: hbase-master
      HBASE_CONF_hbase_master_port: 16000
      HBASE_CONF_hbase_master_info_port: 16010
    volumes:
      - ./config/hbase:/opt/hbase/conf
    networks:
      - hadoop
    depends_on:
      - namenode
      - zookeeper

  hbase-regionserver:
    image: harisekhon/hbase:latest
    container_name: hbase-regionserver
    restart: always
    ports:
      - "16030:16030"
      - "16020:16020"
    environment:
      HBASE_CONF_hbase_rootdir: hdfs://namenode:8020/hbase
      HBASE_CONF_hbase_zookeeper_quorum: zookeeper
      HBASE_CONF_hbase_zookeeper_property_clientPort: 2181
      HBASE_CONF_hbase_cluster_distributed: "true"
      HBASE_CONF_hbase_regionserver_hostname: hbase-regionserver
      HBASE_CONF_hbase_regionserver_port: 16020
      HBASE_CONF_hbase_regionserver_info_port: 16030
    volumes:
      - ./config/hbase:/opt/hbase/conf
    networks:
      - hadoop
    depends_on:
      - hbase-master

  # ==================== ANALYTICS LAYER ====================
  superset:
    image: apache/superset:latest
    container_name: superset
    restart: always
    ports:
      - "8089:8088"
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
      - SUPERSET_SECRET_KEY=mysecretkey123
    volumes:
      - superset_home:/app/superset_home
      - ./superset/superset_config.py:/app/superset_config.py
    networks:
      - hadoop
    depends_on:
      - hive-server

  # ==================== UTILITIES ====================
  jupyter:
    image: jupyter/all-spark-notebook:latest
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=/opt/conda/bin/python
      - PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - spark-master

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  hive_metastore_data:
  superset_home:

networks:
  hadoop:
    driver: bridge
