version: '3.8'

services:
  # ==================== STORAGE LAYER ====================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data:/data
    environment:
      - CLUSTER_NAME=lampung-flood-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    networks:
      - hadoop

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./data:/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_hadoop_proxyuser_hue_hosts: "*"
      CORE_CONF_hadoop_proxyuser_hue_groups: "*"
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_webhdfs_enabled: true
      HDFS_CONF_dfs_permissions_enabled: false
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: false
    networks:
      - hadoop
    depends_on:
      - namenode

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    ports:
      - "8088:8088"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864"
      YARN_CONF_yarn_log___aggregation___enable: true
      YARN_CONF_yarn_log_server_url: http://historyserver:8188/applicationhistory/logs/
      YARN_CONF_yarn_resourcemanager_recovery_enabled: true
      YARN_CONF_yarn_resourcemanager_store_class: org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
      YARN_CONF_yarn_resourcemanager_scheduler_class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: 8192
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: 4
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    ports:
      - "8042:8042"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864 resourcemanager:8088"
      YARN_CONF_yarn_log___aggregation___enable: true
      YARN_CONF_yarn_log_server_url: http://historyserver:8188/applicationhistory/logs/
      YARN_CONF_yarn_resourcemanager_recovery_enabled: true
      YARN_CONF_yarn_resourcemanager_store_class: org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
      YARN_CONF_yarn_resourcemanager_scheduler_class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: 8192
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: 4
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: /app-logs
      YARN_CONF_yarn_nodemanager_aux___services: mapreduce_shuffle
    networks:
      - hadoop
    depends_on:
      - resourcemanager

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    ports:
      - "8188:8188"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864 resourcemanager:8088"
      YARN_CONF_yarn_log___aggregation___enable: true
      YARN_CONF_yarn_log_server_url: http://historyserver:8188/applicationhistory/logs/
      YARN_CONF_yarn_resourcemanager_recovery_enabled: true
      YARN_CONF_yarn_resourcemanager_store_class: org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    networks:
      - hadoop
    depends_on:
      - resourcemanager

  # ==================== STREAMING LAYER ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - hadoop

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    restart: always
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - hadoop
    depends_on:
      - zookeeper

  # ==================== PROCESSING LAYER ====================
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    restart: always
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - hadoop
    depends_on:
      - namenode

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    restart: always
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - hadoop
    depends_on:
      - spark-master

  # ==================== SERVING LAYER ====================
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    restart: always
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    volumes:
      - ./hive/data:/opt/hive/data
    networks:
      - hadoop
    depends_on:
      - hive-metastore

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    restart: always
    ports:
      - "9083:9083"
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864 hive-metastore-postgresql:5432"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql/metastore"
    networks:
      - hadoop
    depends_on:
      - hive-metastore-postgresql

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    restart: always
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    volumes:
      - hive_postgresql_data:/var/lib/postgresql/data
    networks:
      - hadoop
  hbase-master:
    image: bde2020/hbase-master:1.0.0-hbase1.2.6
    container_name: hbase-master
    restart: always
    ports:
      - "16010:16010"
      - "16000:16000"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864 zookeeper:2181"
      HBASE_CONF_hbase_rootdir: hdfs://namenode:8020/hbase
      HBASE_CONF_hbase_zookeeper_quorum: zookeeper
      HBASE_CONF_hbase_zookeeper_property_clientPort: 2181
      HBASE_CONF_hbase_cluster_distributed: true
      HBASE_CONF_hbase_master_hostname: hbase-master
    networks:
      - hadoop
    depends_on:
      - namenode
      - zookeeper
  hbase-regionserver:
    image: bde2020/hbase-regionserver:1.0.0-hbase1.2.6
    container_name: hbase-regionserver
    restart: always
    ports:
      - "16030:16030"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 datanode:9864 zookeeper:2181 hbase-master:16010"
      HBASE_CONF_hbase_rootdir: hdfs://namenode:8020/hbase
      HBASE_CONF_hbase_zookeeper_quorum: zookeeper
      HBASE_CONF_hbase_zookeeper_property_clientPort: 2181
      HBASE_CONF_hbase_cluster_distributed: true
      HBASE_CONF_hbase_regionserver_hostname: hbase-regionserver
    networks:
      - hadoop
    depends_on:
      - hbase-master

  # ==================== ANALYTICS LAYER ====================
  superset:
    image: apache/superset:latest
    container_name: superset
    restart: always
    ports:
      - "8089:8088"
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
    volumes:
      - superset_home:/app/superset_home
      - ./superset/superset_config.py:/app/superset_config.py
    networks:
      - hadoop
    depends_on:
      - hive-server

  # ==================== UTILITIES ====================
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
    networks:
      - hadoop
    depends_on:
      - spark-master

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  hive_postgresql_data:
  superset_home:

networks:
  hadoop:
    driver: bridge