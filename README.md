# ğŸŒŠ Predictive Flood Analytics with Hadoop Ecosystem in Lampung

Welcome to the **Flood Prediction Big Data Project** repository! ğŸš€
This project showcases the integration of **multi-source flood data** using a full-fledged **Apache Hadoop Ecosystem**. The system is designed to support **real-time and batch processing** for flood prediction in **Lampung Province**, Indonesia.

> **Team Members:**  
> Gymnastiar Al Khoarizmy (122450096) | Hermawan Manurung (122450069) | Shula Talitha A P (121450087) | Esteria Rohanauli Sidauruk (122450025)

## ğŸ¯ Project Status: **PRODUCTION READY** âœ…

**Recent Updates (May 2025):**
- âœ… **Fixed ResourceManager restart loop issue** - YARN cluster now stable
- âœ… **Resolved HBase RegionServer standalone mode** - Full distributed configuration
- âœ… **Enhanced service startup dependencies** - Sequential startup prevents conflicts  
- âœ… **All web UIs accessible** - Complete monitoring and management interfaces
- âœ… **Production-grade Docker environment** - Ready for analytics workloads

---

## ğŸ”§ Installation & Setup

### Prerequisites
- Docker and Docker Compose installed
- Git
- At least 8GB RAM available for Docker

### Quick Start

1. Clone the repository:
   ```bash
   git clone https://github.com/sains-data/Analisis-Prediksi-Banjir.git
   cd Analisis-Prediksi-Banjir
   ```

2. Initialize the system (formats namenode and starts all services):
   ```bash
   chmod +x scripts/init-namenode.sh
   bash ./scripts/init-namenode.sh
   docker-compose up -d
   ```

3. Verify all services are running:
   ```bash
   docker-compose ps
   ```

4. Access web interfaces:
   - HDFS NameNode: `http://localhost:9870`
   - YARN ResourceManager: `http://localhost:8088`
   - Spark Master: `http://localhost:8080`
   - Jupyter Notebook: `http://localhost:8888`
   - Superset: `http://localhost:8089`
   - HBase Master: `http://localhost:16010`

### Troubleshooting

If you encounter issues with the namenode not starting properly:
```bash
# Stop all containers
docker-compose down

# Run the init script again
./scripts/init-namenode.sh
```

---

## ğŸ§± System Architecture

We adopt a **multi-layered architecture** and hybrid processing approach:


| Layer                | Description                         | Tools                         | Format             |
| -------------------- | ----------------------------------- | ----------------------------- | ------------------ |
| **Raw Data Layer**   | Stores raw data from all sources    | Kafka, Flume, HDFS            | CSV, JSON, GeoTIFF |
| **Processing Layer** | ETL, transformation, model training | Spark, Spark Streaming, MLlib | Parquet, Avro      |
| **Serving Layer**    | Ready-to-query structured data      | Hive, HBase                   | ORC, Parquet       |
| **Analytics Layer**  | Visual dashboards and early alerts  | Superset, Kafka               |  -                  |

---

## ğŸ“– Project Overview

This project includes:

1. **Hybrid Pipeline**: Batch + Streaming for multi-source flood data
2. **Machine Learning**: Flood prediction with Spark MLlib
3. **IoT Integration**: Real-time sensor data via Kafka & HBase
4. **BI & Alerting**: Dashboard + early warning system via Superset

ğŸŒŸ Key Focus Areas:

* Apache Hadoop Distributed File System
* Apache Spark (MLlib, Streaming)
* Apache Kafka & Hive
* Data Modeling for Streaming & Batch
* Docker-based Orchestration (Airflow, Docker Compose)

---

## âš™ï¸ System Components

### ğŸ§¹ Tech Stack

| Category            | Tools                  |
| ------------------- | ---------------------- |
| Distributed Storage | Hadoop HDFS            |
| Batch Processing    | Apache Spark           |
| Stream Processing   | Spark Streaming, Kafka |
| Query Layer         | Hive, Spark SQL        |
| Data Lake Store     | Parquet, ORC           |
| ML                  | Spark MLlib            |
| IoT Data            | HBase, Kafka           |
| Orchestration       | Apache Airflow         |
| Monitoring          | Apache Ambari          |
| Dashboard           | Apache Superset        |

---

## ğŸ”„ Workflow DAG (Airflow)

```
flood_prediction_pipeline/
â”œâ”€â”€ ingest_bmkg_data
â”œâ”€â”€ ingest_bnpb_data
â”œâ”€â”€ process_demnas_geotiff
â”œâ”€â”€ load_data_to_hdfs
â”œâ”€â”€ spark_data_cleaning
â”œâ”€â”€ feature_engineering
â”œâ”€â”€ model_training
â”œâ”€â”€ generate_risk_map
â”œâ”€â”€ hive_refresh
â””â”€â”€ notify_stakeholders
```

---

## ğŸ“¦ Folder Structure

```
Analisis-Prediksi-Banjir/
â””â”€â”€ Analisis-Prediksi-Banjir/
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ hive-server-entrypoint.sh
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ test_mapreduce.sh
    â”œâ”€â”€ .git/
    â”‚   â”œâ”€â”€ config, HEAD, index, etc.
    â”‚   â””â”€â”€ ... (standard Git repo files and structure)
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ hadoop/
    â”‚   â”‚   â”œâ”€â”€ core-site.xml
    â”‚   â”‚   â”œâ”€â”€ hdfs-site.xml
    â”‚   â”‚   â”œâ”€â”€ mapred-site.xml
    â”‚   â”‚   â””â”€â”€ yarn-site.xml
    â”‚   â”œâ”€â”€ hbase/
    â”‚   â”‚   â””â”€â”€ hbase-site.xml
    â”‚   â”œâ”€â”€ hive/
    â”‚   â”‚   â”œâ”€â”€ hive-site.xml
    â”‚   â”‚   â””â”€â”€ simple-hive-site.xml
    â”‚   â””â”€â”€ spark/
    â”‚       â””â”€â”€ spark-defaults.conf
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ processed/
    â”‚   â”‚   â””â”€â”€ .gitkeep
    â”‚   â”œâ”€â”€ raw/
    â”‚   â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â”‚   â”œâ”€â”€ bmkg/cuaca_historis/data_cuaca_bmkg.csv
    â”‚   â”‚   â”œâ”€â”€ bnpb/kejadian_banjir/data_banjir_historis.csv
    â”‚   â”‚   â”œâ”€â”€ demnas/topografi/data_elevasi_demnas.csv
    â”‚   â”‚   â””â”€â”€ iot/data_sensor_iot.json
    â”‚   â”œâ”€â”€ sample/
    â”‚   â”‚   â””â”€â”€ .gitkeep
    â”‚   â””â”€â”€ serving/
    â”‚       â””â”€â”€ .gitkeep
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ hadoop/
    â”‚   â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.datanode
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.namenode
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.resourcemanager
    â”‚   â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”‚   â”œâ”€â”€ core-site.xml
    â”‚   â”‚   â”‚   â”œâ”€â”€ hadoop-env.sh
    â”‚   â”‚   â”‚   â”œâ”€â”€ hdfs-site.xml
    â”‚   â”‚   â”‚   â”œâ”€â”€ mapred-site.xml
    â”‚   â”‚   â”‚   â””â”€â”€ yarn-site.xml
    â”‚   â”‚   â””â”€â”€ scripts/
    â”‚   â”‚       â”œâ”€â”€ entrypoint-datanode.sh
    â”‚   â”‚       â”œâ”€â”€ entrypoint-namenode.sh
    â”‚   â”‚       â””â”€â”€ entrypoint-resourcemanager.sh
    â”‚   â”œâ”€â”€ hbase/
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.master
    â”‚   â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”‚   â”œâ”€â”€ hbase-env.sh
    â”‚   â”‚   â”‚   â””â”€â”€ hbase-site.xml
    â”‚   â”‚   â””â”€â”€ scripts/
    â”‚   â”‚       â””â”€â”€ entrypoint-hbase-master.sh
    â”‚   â”œâ”€â”€ hive/
    â”‚   â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.hive
    â”‚   â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”‚   â”œâ”€â”€ hive-env.sh
    â”‚   â”‚   â”‚   â””â”€â”€ hive-site.xml
    â”‚   â”‚   â””â”€â”€ scripts/
    â”‚   â”‚       â””â”€â”€ entrypoint-hive.sh
    â”‚   â”œâ”€â”€ kafka/
    â”‚   â”‚   â””â”€â”€ .gitkeep
    â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â””â”€â”€ .gitkeep
    â”‚   â”œâ”€â”€ spark/
    â”‚   â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.master
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.worker
    â”‚   â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”‚   â”œâ”€â”€ spark-defaults.conf
    â”‚   â”‚   â”‚   â””â”€â”€ spark-env.sh
    â”‚   â”‚   â””â”€â”€ scripts/
    â”‚   â”‚       â”œâ”€â”€ entrypoint-spark-master.sh
    â”‚   â”‚       â””â”€â”€ entrypoint-spark-worker.sh
    â”‚   â””â”€â”€ zookeeper/
    â”‚       â”œâ”€â”€ Dockerfile
    â”‚       â”œâ”€â”€ config/
    â”‚       â”‚   â””â”€â”€ zoo.cfg
    â”‚       â””â”€â”€ scripts/
    â”‚           â””â”€â”€ entrypoint-zookeeper.sh
    â”œâ”€â”€ hive/
    â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â””â”€â”€ data/metastore/.gitkeep
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â””â”€â”€ hive_spark_integration_test.ipynb
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ backup_system.sh
    â”‚   â”œâ”€â”€ init-namenode.sh
    â”‚   â”œâ”€â”€ init_system.sh
    â”‚   â”œâ”€â”€ stop.sh
    â”‚   â”œâ”€â”€ test_pipeline.py
    â”‚   â”œâ”€â”€ analytics/
    â”‚   â”‚   â””â”€â”€ .gitkeep
    â”‚   â”œâ”€â”€ ingestion/
    â”‚   â”‚   â”œâ”€â”€ .gitkeep
    â”‚   â”‚   â”œâ”€â”€ bmkg_ingestion.py
    â”‚   â”‚   â””â”€â”€ ingest_bmkg.py
    â”‚   â”œâ”€â”€ ml/
    â”‚   â”‚   â””â”€â”€ flood_prediction_model.py
    â”‚   â””â”€â”€ processing/
    â”‚       â””â”€â”€ .gitkeep
    â”œâ”€â”€ spark/
    â”‚   â””â”€â”€ data/
    â”‚       â””â”€â”€ .gitkeep
    â””â”€â”€ superset/
        â””â”€â”€ superset_config.py

```

---

## ğŸš€ How to Run the Project (Deployment)

1. Clone the repo:

   ```bash
   git clone https://github.com/your-repo/flood-bigdata-lampung.git
   cd flood-bigdata-lampung
   ```

2. Start the cluster:

   ```bash
   docker-compose up -d
   ```

3. Access services:

   * Hadoop UI: `localhost:9870`
   * Spark UI: `localhost:4040`
   * Hive: `localhost:10000`
   * Superset: `localhost:8088`
   * Airflow: `localhost:8080`

4. Load sample data into `/data/raw/` and trigger Airflow DAG.

---

## ğŸ“Š Dashboard Preview

<p align="center">
  <img src="docs/superset_dashboard.png" width="800px"/>
</p>

---

## ğŸ›¡ï¸ Requirements & Functional Specs

### âœ… Functional Requirements

* Ingest BMKG, BNPB, and sensor data into HDFS
* Stream IoT sensor data using Kafka â†’ Spark Streaming
* Train flood prediction model with Spark MLlib
* Provide SQL interface with Hive
* Trigger early warning alerts
* Generate flood risk maps

### âš™ï¸ Non-Functional Requirements

* High availability and scalability
* Max streaming latency: 5 minutes
* Access control per user role
* Efficient storage with Parquet/ORC
* Dockerized for easy deployment

---

## ğŸ  Sample Use Case: Bandar Lampung

On **11 June 2020**, Kalibalau River overflowed, causing urban flooding. This system integrates:

* ğŸŒ§ï¸ BMKG weather data
* ğŸ¤­ DEMNAS elevation data
* ğŸ’§ IoT sensor water level
* ğŸ“Š Historical flood incidents

> Result: Real-time analytics and accurate flood predictions help mitigate disaster impact.

---

## â˜ï¸ Sample Dataset Sources

* BMKG: Rainfall, humidity, temperature
* BNPB: Historical flood reports
* DEMNAS: Digital Elevation Maps
* IoT: Local sensors from BPBD

---

## ğŸ“¬ Contact & Credits

Developed by **Kelompok 6** â€“ Institut Teknologi Sumatera
