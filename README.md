# ğŸŒŠ Predictive Flood Analytics with Hadoop Ecosystem in Lampung

Welcome to the **Flood Prediction Big Data Project** repository! ğŸš€
This project showcases the integration of **multi-source flood data** using a full-fledged **Apache Hadoop Ecosystem**. The system is designed to support **real-time and batch processing** for flood prediction in **Lampung Province**, Indonesia.

> Gymnastiar Al Khoarizmy (122450096) | Hermawan Manurung (122450069) | Shula Talitha Ardhya Putri (121450087) | Esteria Rohanauli S (122450025)
---

## ğŸ§± System Architecture

We adopt a **multi-layered architecture** and hybrid processing approach:


| Layer                | Description                         | Tools                         | Format             |
| -------------------- | ----------------------------------- | ----------------------------- | ------------------ |
| **Raw Data Layer**   | Stores raw data from all sources    | Kafka, Flume, HDFS            | CSV, JSON, GeoTIFF |
| **Processing Layer** | ETL, transformation, model training | Spark, Spark Streaming, MLlib | Parquet, Avro      |
| **Serving Layer**    | Ready-to-query structured data      | Hive, HBase                   | ORC, Parquet       |
| **Analytics Layer**  | Visual dashboards and early alerts  | Superset, Kafka               | -                  |

---

## ğŸ“– Project Overview

This project includes:

1. **Hybrid Pipeline**: Batch + Streaming for multi-source flood data
2. **Machine Learning**: Flood prediction with Spark MLlib
3. **IoT Integration**: Real-time sensor data via Kafka & HBase
4. **BI & Alerting**: Dashboard + early warning system via Superset

ğŸŒŸ Key Focus Areas:

* Apache Hadoop Distributed File System
* Apache Spark (MLlib, Streaming)
* Apache Kafka & Hive
* Data Modeling for Streaming & Batch
* Docker-based Orchestration (Airflow, Docker Compose)

---

## âš™ï¸ System Components

### ğŸ§¹ Tech Stack

| Category            | Tools                  |
| ------------------- | ---------------------- |
| Distributed Storage | Hadoop HDFS            |
| Batch Processing    | Apache Spark           |
| Stream Processing   | Spark Streaming, Kafka |
| Query Layer         | Hive, Spark SQL        |
| Data Lake Store     | Parquet, ORC           |
| ML                  | Spark MLlib            |
| IoT Data            | HBase, Kafka           |
| Orchestration       | Apache Airflow         |
| Monitoring          | Apache Ambari          |
| Dashboard           | Apache Superset        |

---

## ğŸ”„ Workflow DAG (Airflow)

```
flood_prediction_pipeline/
â”œâ”€â”€ ingest_bmkg_data
â”œâ”€â”€ ingest_bnpb_data
â”œâ”€â”€ process_demnas_geotiff
â”œâ”€â”€ load_data_to_hdfs
â”œâ”€â”€ spark_data_cleaning
â”œâ”€â”€ feature_engineering
â”œâ”€â”€ model_training
â”œâ”€â”€ generate_risk_map
â”œâ”€â”€ hive_refresh
â””â”€â”€ notify_stakeholders
```

---

## ğŸ“¦ Folder Structure

```
flood-bigdata-lampung/
â”‚
â”œâ”€â”€ datasets/                    # Raw and sample flood datasets (BMKG, BNPB, IoT, DEMNAS)
â”œâ”€â”€ docs/                        # Diagrams, specs, and documentation
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ data_catalog.md
â”‚   â”œâ”€â”€ pipeline.drawio
â”‚   â””â”€â”€ dag_airflow.drawio
â”œâ”€â”€ scripts/                     # Spark, Hive, and Kafka scripts
â”‚   â”œâ”€â”€ batch/
â”‚   â”œâ”€â”€ stream/
â”‚   â””â”€â”€ ml/
â”œâ”€â”€ docker/                      # Docker & Docker Compose setup
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ cluster-config/
â”œâ”€â”€ notebooks/                   # Jupyter analysis notebooks
â”œâ”€â”€ airflow_dags/               # DAGs for pipeline orchestration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸš€ How to Run the Project (Deployment)

1. Clone the repo:

   ```bash
   git clone https://github.com/your-repo/flood-bigdata-lampung.git
   cd flood-bigdata-lampung
   ```

2. Start the cluster:

   ```bash
   docker-compose up -d
   ```

3. Access services:

   * Hadoop UI: `localhost:9870`
   * Spark UI: `localhost:4040`
   * Hive: `localhost:10000`
   * Superset: `localhost:8088`
   * Airflow: `localhost:8080`

4. Load sample data into `/data/raw/` and trigger Airflow DAG.

---

## ğŸ“Š Dashboard Preview

<p align="center">
  <img src="docs/superset_dashboard.png" width="800px"/>
</p>

---

## ğŸ›¡ï¸ Requirements & Functional Specs

### âœ… Functional Requirements

* Ingest BMKG, BNPB, and sensor data into HDFS
* Stream IoT sensor data using Kafka â†’ Spark Streaming
* Train flood prediction model with Spark MLlib
* Provide SQL interface with Hive
* Trigger early warning alerts
* Generate flood risk maps

### âš™ï¸ Non-Functional Requirements

* High availability and scalability
* Max streaming latency: 5 minutes
* Access control per user role
* Efficient storage with Parquet/ORC
* Dockerized for easy deployment

---

## ğŸ  Sample Use Case: Bandar Lampung

On **11 June 2020**, Kalibalau River overflowed, causing urban flooding. This system integrates:

* ğŸŒ§ï¸ BMKG weather data
* ğŸ¤­ DEMNAS elevation data
* ğŸ’§ IoT sensor water level
* ğŸ“Š Historical flood incidents

> Result: Real-time analytics and accurate flood predictions help mitigate disaster impact.

---

## â˜ï¸ Sample Dataset Sources

* BMKG: Rainfall, humidity, temperature
* BNPB: Historical flood reports
* DEMNAS: Digital Elevation Maps
* IoT: Local sensors from BPBD

---

## ğŸ“¬ Contact & Credits

Developed by **Kelompok 6** â€“ Institut Teknologi Sumatera
