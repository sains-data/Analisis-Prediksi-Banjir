{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc48536d",
   "metadata": {},
   "source": [
    "# HDFS and Hive Integration Test\n",
    "\n",
    "This notebook tests the integration between Hadoop (HDFS), Spark, and Hive in our data processing ecosystem, which was recently fixed to use Derby as the metastore database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05078b3",
   "metadata": {},
   "source": [
    "# Spark-Hive Integration Test\n",
    "\n",
    "This notebook tests the integration between Apache Spark and Apache Hive in our data processing ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5823dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveTest\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-server:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6707c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show databases\n",
    "print(\"Databases in Hive:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test table if it doesn't exist\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark_jupyter_test (id INT, name STRING)\")\n",
    "\n",
    "# Insert some test data\n",
    "spark.sql(\"INSERT INTO spark_jupyter_test VALUES (1, 'test from jupyter')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "print(\"Data in spark_jupyter_test:\")\n",
    "spark.sql(\"SELECT * FROM spark_jupyter_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to query the test_table we created earlier with beeline\n",
    "print(\"Data in test_table:\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM test_table\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e575bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all tables\n",
    "print(\"All tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422b1fd",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Test basic Spark functionality\n",
    "import pyspark\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "\n",
    "# Initialize basic Spark session first\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"TestSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print Spark version\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfbd3d",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Test HDFS access\n",
    "try:\n",
    "    # Read the test file we created in HDFS\n",
    "    hdfs_file = spark.read.text(\"hdfs://namenode:8020/hdfs_test.txt\")\n",
    "    print(\"HDFS test file content:\")\n",
    "    hdfs_file.show()\n",
    "    print(\"✅ HDFS integration is working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading HDFS file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83cba2",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the current session and create one with Hive support\n",
    "spark.stop()\n",
    "\n",
    "# Create a new Spark session with Hive support\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"HiveTest\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test Hive integration\n",
    "try:\n",
    "    # Show available databases\n",
    "    print(\"Available databases:\")\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    \n",
    "    # Show available tables\n",
    "    print(\"\\nAvailable tables:\")\n",
    "    spark.sql(\"SHOW TABLES\").show()\n",
    "    \n",
    "    print(\"\\n✅ Hive integration is working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with Hive integration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36722d",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Test creating and querying a Hive table\n",
    "try:\n",
    "    # Create a test table\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS jupyter_test (id INT, name STRING)\")\n",
    "    \n",
    "    # Insert test data\n",
    "    spark.sql(\"INSERT INTO jupyter_test VALUES (1, 'jupyter test')\")\n",
    "    \n",
    "    # Query the table\n",
    "    print(\"Data in jupyter_test:\")\n",
    "    spark.sql(\"SELECT * FROM jupyter_test\").show()\n",
    "    \n",
    "    print(\"\\n✅ Hive table creation and querying is working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with Hive table operations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be25a6",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Test completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
