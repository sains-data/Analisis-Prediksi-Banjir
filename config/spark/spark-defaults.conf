# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ===============================================================
# SPARK 3.5.4 CONFIGURATION FOR FLOOD PREDICTION ANALYTICS
# ===============================================================

# Master Configuration
spark.master                     spark://spark-master:7077
spark.app.name                   FloodPredictionAnalytics

# Driver Configuration
spark.driver.memory              1g
spark.driver.cores               1
spark.driver.maxResultSize       512m
spark.driver.port               7001
spark.driver.host               spark-master

# Executor Configuration
spark.executor.memory            1g
spark.executor.cores             1
spark.executor.instances         2
spark.executor.heartbeatInterval 10s
spark.executor.extraJavaOptions  -XX:+UseG1GC

# Network Configuration
spark.network.timeout            800s
spark.rpc.askTimeout             600s
spark.rpc.lookupTimeout          120s

# Storage Configuration
spark.storage.memoryFraction     0.6
spark.storage.level              MEMORY_AND_DISK_SER
spark.storage.unrollFraction     0.2

# Shuffle Configuration
spark.shuffle.compress           true
spark.shuffle.spill.compress     true
spark.shuffle.file.buffer        32k
spark.shuffle.io.retryWait       5s
spark.shuffle.io.maxRetries      5

# Serialization Configuration
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired  false
spark.kryo.unsafe                false

# SQL Configuration
spark.sql.adaptive.enabled                    true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled           true
spark.sql.warehouse.dir                       hdfs://namenode:8020/user/hive/warehouse
spark.sql.catalogImplementation               hive

# Hive Integration
spark.sql.hive.metastore.version            4.0.1
spark.sql.hive.metastore.jars                builtin
spark.hadoop.hive.metastore.uris             thrift://hive-metastore:9083

# HDFS Configuration
spark.hadoop.fs.defaultFS                   hdfs://namenode:8020
spark.hadoop.dfs.nameservices               lampung-flood-cluster
spark.hadoop.dfs.client.use.datanode.hostname true

# YARN Integration (optional, for hybrid deployment)
spark.yarn.am.memory                         512m
spark.yarn.am.cores                          1
spark.yarn.executor.memoryFraction           0.8

# Dynamic Allocation (disabled for fixed cluster)
spark.dynamicAllocation.enabled              false
spark.dynamicAllocation.minExecutors         1
spark.dynamicAllocation.maxExecutors         4
spark.dynamicAllocation.initialExecutors     2

# Event Log Configuration
spark.eventLog.enabled                       true
spark.eventLog.dir                          hdfs://namenode:8020/var/log/spark-events
spark.history.fs.logDirectory               hdfs://namenode:8020/var/log/spark-events

# UI Configuration
spark.ui.enabled                             true
spark.ui.port                               4040
spark.ui.killEnabled                        true
spark.ui.retainedJobs                       100
spark.ui.retainedStages                     100

# Monitoring and Metrics
spark.metrics.conf                          /opt/spark/conf/metrics.properties
spark.sql.streaming.metricsEnabled          true

# Security Configuration (disabled for development)
spark.authenticate                           false
spark.acls.enable                           false
spark.admin.acls                            *

# Big Data Optimizations for Flood Analysis
spark.sql.files.maxPartitionBytes          134217728  # 128MB
spark.sql.files.openCostInBytes            4194304    # 4MB
spark.sql.broadcastTimeout                 300        # 5 minutes
spark.sql.execution.arrow.pyspark.enabled  true

# Kafka Integration for Streaming
spark.jars.packages                         org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4,org.apache.kafka:kafka-clients:3.8.1

# Machine Learning Configuration
spark.ml.python.worker.reuse                true
spark.python.worker.reuse                   true
spark.python.worker.memory                  512m

# Checkpoint Configuration
spark.sql.streaming.checkpointLocation      hdfs://namenode:8020/checkpoints/streaming

# Local directories for temporary files
spark.local.dir                             /tmp/spark-temp

# JVM Options
spark.driver.extraJavaOptions              -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintRegionRememberSetInfo -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16m
spark.executor.extraJavaOptions            -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintRegionRememberSetInfo -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16m

# Environment Variables
spark.executorEnv.PYTHONHASHSEED            0
spark.executorEnv.PYSPARK_PYTHON            python3
